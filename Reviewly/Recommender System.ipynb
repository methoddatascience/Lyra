{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Overview: Reviewly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewly is a startup company that has retained our consulting services to build a <b>\"User-Item Collaborative Filtering\" recommender system (RS)</b>.  \n",
    "\n",
    "User-Item Collaborative Filtering: “<i>Customers who are similar to you also liked …</i>”\n",
    "\n",
    "_(Ideally, I think implicit cues should be used instead of explicit ratings, but... that's not this project!  --Jess)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person A as input --> identify people who are similar to Person A based on similar ratings --> make recommendations for Person A based on predicted ratings of similar people\n",
    "\n",
    "Can be conceptualized as a large, sparsely filled (? we need more info on this) matrix with raters (users) across the top and items (being rated) down the side.  Each cell contains an observed rating (scale TBD) for an item (row) by a specific user (column) OR is blank.  We assume that only the most current rating should be considered, so we don't need to track ratings that are changed/updated by the user at different points in time.  Accuracy can be judged by root mean squared error (RMSE).\n",
    "\n",
    "1) Average rating for an item\n",
    "2) User bias\n",
    "3) User similarity to raters\n",
    "\n",
    "```\n",
    "# sample logic for average ratings and user bias\n",
    "\n",
    "rating_mean = by_rating_mean['stars'].tolist()\n",
    "user_bias = df_sparse.T.apply(lambda k: k.sum()/(k != 0).sum()).tolist()\n",
    "\n",
    "# sample logic for predicting user similarity to other raters\n",
    "# \"fans\" rate the item above the mean, \"haters\" rate it below the mean\n",
    "\n",
    "def user_fan_similarity(user, item):\n",
    "    similarity_vals_list = []\n",
    "    fans = np.where(data.iloc[:, item] > data.iloc[:, item].mean())[0]\n",
    "    for fan in fans:\n",
    "        similarity_vals_list.append(user_similarity(user, fan))\n",
    "        return sum(similarity_vals_list) / float(len(similarity_vals_list))\n",
    "        \n",
    "for item in range(len(item_list)):\n",
    "    item_mean_rating = rating_means[item]\n",
    "    user_correlation_with_fan_adjustment = user_rating_matrix[item]\n",
    "    predicted_rating = item_mean_rating + bias score + user_correlation_with_fan_adjustment\n",
    "    print(\"Predicted rating for item %d = %.2f\" % (item, predicted_rating))\n",
    "    rating_predictions.append(predicted_rating)\n",
    "```\n",
    "\n",
    "Need to determine the appropriate categories/clusters for the items once we know what is being rated.  Then, continue with appropriate modeling or ML technique.\n",
    "\n",
    "```\n",
    "# SVD (singular value decomposition)\n",
    "file = File()\n",
    "\n",
    "# example with 100 rows\n",
    "data = Dataset.load_from_df(df[['User_Id', 'Item_Id', 'Rating']][:100], file)\n",
    "data.split(n_folds=3)\n",
    "\n",
    "svd = SVD()\n",
    "evaluate(svd, data, measures=['RMSE', 'MAE'])\n",
    "\n",
    "# What Person A liked most (assuming \"5\" as highest rating)\n",
    "\n",
    "df_PersonA = df[(df['User_Id'] == PersonA) & (df['Rating'] == 5)]\n",
    "df_PersonA = df_PersonA.set_index('Item_Id')\n",
    "df_PersonA = df_PersonA.join(df_item)['Name']\n",
    "print(df_PersonA)\n",
    "\n",
    "# PREDICT what Person A would like based on what they liked before using SVD\n",
    "\n",
    "data = Dataset.load_from_df(df[['User_Id', 'Item_Id', 'Rating']], file)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "svd.train(trainset)\n",
    "\n",
    "PersonA['Estimate_Rating'] = PersonA['Item_Id'].apply(lambda x: svd.predict(PersonA, x).est)\n",
    "\n",
    "PersonA = PersonA.drop('Item_Id', axis = 1)\n",
    "\n",
    "PersonA = PersonA.sort_values('Estimate_Rating', ascending=False)\n",
    "print(PersonA.head(10))\n",
    "\n",
    "# RECOMMEND an item for Person A based on the predictions of what they would like\n",
    "\n",
    "# Use Pearsons' R correlation to measure the linear correlation between ratings of \n",
    "# all pairs of items, then recommend the top 10 items with highest correlations\n",
    "\n",
    "def recommend(item_name, min_count):\n",
    "    print(\"For item ({})\".format(item_name))\n",
    "    print(\"- Top 10 items recommended based on Pearsons'R correlation - \")\n",
    "    i = int(df_item.index[df_item['Name'] == item_name][0])\n",
    "    target = df_p[i]\n",
    "    similar_to_target = df_p.corrwith(target)\n",
    "    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])\n",
    "    corr_target.dropna(inplace = True)\n",
    "    corr_target = corr_target.sort_values('PearsonR', ascending = False)\n",
    "    corr_target.index = corr_target.index.map(int)\n",
    "    corr_target = corr_target.join(df_item).join(df_item_summary)[['PearsonR', 'Name', 'count', 'mean']]\n",
    "    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))\n",
    "    \n",
    "# Based on some input (e.g. an item name), recommend \"top 10\" most likely to be liked by Person A\n",
    "recommend(\"Some item\", 0)\n",
    "```\n",
    "\n",
    "Need to extend this logic taking into account what OTHER raters similar to Person A (as per first block of pseudocode) liked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data source with user ratings\n",
    "    * where ratings are not available, \"implicit\" proxies for ratings can be used (e.g. other purchases, website behavior)- see important unknowns\n",
    "* Appropriate categories/clusters for the items being rated\n",
    "* Decision about whether to consider items with few ratings and users with few ratings (unless new) in algorithm, which will affect size of the \"matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Users don't like to rate things\n",
    "* What is the \"psychology of rating\"?  Do users tend to rate things only at the extremes (very good or very bad)?\n",
    "* Privacy regulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Unknowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of existing user ratings\n",
    "* Source of user ratings\n",
    "* Quality of user ratings\n",
    "* Type of ratings \n",
    "* Format of ratings\n",
    "* Context in which the RS will be used\n",
    "* Target market\n",
    "* Client technology systems (how will this be deployed/operationalized?)\n",
    "* Timeline\n",
    "* Resources\n",
    "* Existing systems/algorithms\n",
    "* Company expertise\n",
    "* Market competition\n",
    "* Constraints/limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link>http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "<link>https://en.wikipedia.org/wiki/Collaborative_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
